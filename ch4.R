###### Chapter 4 Prediction, r-squared, and modeling
#### Forecasting
alpha <- 0.05
x <- 20
xbar <- mean(food$income) # get xbar
b1 <- coef(mod1)[[1]]
b2 <- coef(mod1)[[2]]
yhatx <- b1+b2*x # get yhat
#
df <- df.residual(mod1)
tc <- qt(1-alpha/2, df)
#
N <- nobs(mod1) # get the number of obeservations used in the model, and counted as sample
# size 
# N <- NROW(mod1)
# N <- nrow(mod1) # other ways to find the sample size
varb2 <- vcov(mod1)[2, 2]
sighat2 <- smod1$sigma^2
#
varf <- sighat2 + sighat2/N + (x-xbar)^2 * varb2 # get the forecast variance
sef <- sqrt(varf) # standard error of the forecast
lb <- yhatx - tc*sef
ub <- yhatx + tc*sef
#
varf; sef; lb; ub # larger than the confidence interval of the estimated expected 
# value of y
##
sef <- sqrt(sighat2 + sighat2/N + (food$income - xbar)^2 * varb2) # get standard
# error of prediction for all the observations in the sample
yhatv <- fitted.values(mod1) # get predicted y given all the x observations 
lbv <- yhatv - tc*sef
ubv <- yhatv + tc*sef
#
xincome <- food$income
dplot <- data.frame(xincome, yhatv, lbv, ubv)
dplotord <- dplot[order(xincome),]
xmax <- max(dplotord$xincome)
xmin <- min(dplotord$xincome)
ymax <- max(dplotord$yhatv)
ymin <- min(dplotord$yhatv)
#
plot(dplotord$xincome, dplotord$yhatv, 
     xlim = c(xmin, xmax),
     ylim = c(ymin, ymax),
     xlab = "income",
     ylab = "food expenditure",
     type = "l",
     main = "Forecast confidence interval for food expenditure")
lines(dplotord$ubv ~ dplotord$xincome, lty = 2, col = "red")
lines(dplotord$lbv ~ dplotord$xincome, lty = 2, col = "red") # shows the confidence 
# interval band about the regression line
##
incomex = data.frame(income = 20)
predict(mod1, newdata = incomex, interval = "confidence", level = 0.95) # for predicted
# model
lowbL;upbL
#
predict(mod1, newdata = incomex, interval = "prediction", level = 0.95) # for forecast
# model
lb;ub
##
xmin <- min(food$income)
xmax <- max(food$income)
income <- seq(xmin, xmax)
ypredict <- predict(mod1, data.frame(income), interval = "confidence")
yforecast <- predict(mod1, data.frame(income), interval = "predict")
#
matplot(income, cbind(ypredict[,1], ypredict[,2], ypredict[,3], 
                      yforecast[,2], yforecast[,3]),
        type = "l", lty = c(1, 2, 2, 3, 3),
        col = c("black", "red", "red", "blue", "blue"),
        ylab = "food expenditure", xlab = "income")
points(food$income, food$food_exp)
legend("topleft", 
       legend = c("E[y|x]", "lwr_pred", "upr_pred", "lwr_forcst", "upr_forcst"),
       lty = c(1, 2, 2, 3, 3),
       col = c("black", "red", "red", "blue", "blue"),
       x.intersp = 0.3, y.intersp = 0.3, seg.len = 0.8)
# forecast band is much larger than the predicted one.
#### Goodness-of-fit
rsq <- smod1$r.squared
smod1
#
anov <- anova(mod1)
dfr <- data.frame(anov)
kable(dfr, caption = "Output generated by the 'anova' function")
#
SSE <- anov[2,2]
SSR <- anov[1,2]
SST <- SSE + SSR
#
df <- anov[2,1]
#### Linear-log models
mod2 <- lm(food_exp ~ log(income), food)
tbl <- data.frame(xtable(mod2))  
kable(tbl, digits = 5, caption = "Linear-log model putput for the *food* example")
#
b1 <- coef(mod2)[[1]]
b2 <- coef(mod2)[[2]]
pmod2 <- predict(mod2, data.frame(income), interval = "confidence")
plot(food$income, food$food_exp,
     xlab = "income",
     ylab = "food expenditure")
lines(pmod2[,1] ~ income, lty = 1, col = "black")
lines(pmod2[,2] ~ income, lty = 2, col = "red")
lines(pmod2[,3] ~ income, lty = 2, col = "red")
#
x <- 10
y <- b1+b2*log(x)
DyDx <- b2/x # get the marginal effect, an unit increase in income will increase the 
# expenditure by $13.216
DyPDx <- b2/100 # semi-elasticity, a 1% increase in income, expenditure increases by 
# $ 1.32
PDyPDx <- b2/y # elasticity, a 1% increase in income, expenditure increases by 0.638%
#
DyDx; DyPDx; PDyPDx
#### Residuals and Diagnostics
ehat <- mod2$residuals
plot(food$income, ehat, xlab = "income", ylab = "residuals")
## testing homoscedasticity
# residual plot with simulated model
set.seed(123)
x <- runif(300, 0, 10)
e <- rnorm(300)
y <- 1 + x + e
mod3 <- lm(y ~ x)
plot(x, mod3$residuals, xlab = "x", ylab = "residuals") # homoscedasticity
# linear quadratic form 
set.seed(123)
x <- runif(300, -2.5, 2.5)
e <- rnorm(300, 0, 4)  
y <- 15 - 4*x^2 + e
mod3 <- lm(y ~ x) # incorrectly specified econometric model
ehat <- resid(mod3)
plot(x, resid(mod3), ylim = c(min(ehat), max(ehat)),
     xlab = "x", ylab = "residuals",
     main = "Simulated quadratic residuals from an incorrectly specified econometric model")
# the second way
car::ncvTest(mod3)
# the third way
car::spreadLevelPlot(mod3)
car::slp(mod3)
## testing normality hypothesis
ehat <- resid(mod1)
ebar <- mean(ehat)
sde <- sd(ehat)
#
hist(ehat, col = "grey", main = "", freq = FALSE, ylab = "density", xlab = "ehat")
curve(dnorm(x, ebar, sde), col = 2, add = TRUE, ylab = "density", xlab = "ehat")
# second method
tseries::jarque.bera.test(ehat)
# third method
car::qqPlot(mod1)
# forth method
car::residualPlot(mod1)
# fifth method
qqnorm(ehat)
qqline(ehat)
## testing independence
car::durbinWatsonTest(mod1)
## testing linearity
car::crPlots(mod1)
## global validation of linear regression assumptions
gvlma::gvlma(mod1)
#### Polynomial models
data("wa_wheat")
View(wa_wheat)
#
mod1 <- lm(greenough ~ time, wa_wheat)
ehat <- resid(mod1)
plot(wa_wheat$time, ehat, xlab = "time", ylab = "residuals")
#
mod2 <- lm(greenough ~ I(time^3), wa_wheat)
ehat <- resid(mod2)
plot(wa_wheat$time, ehat, xlab = "time", ylab = "residuals") # more evenly spread about
# the zero line
#### Log-linear models
mod4 <- lm(log(greenough) ~ time, wa_wheat)
smod4 <- summary(mod4)
tbl <- data.frame(xtable(smod4))
kable(tbl, caption = "Log-linear model for the *yield* equation")
# wheat production has increased at an average rate of approximate 1.78% per year
##
data("cps4_small")
View(cps4_small)
#
xeduc <- 12
mod5 <- lm(log(wage) ~ educ, cps4_small)
smod5 <- summary(mod5)
tbl <- round(data.frame(xtable(smod5)), 3)
kable(tbl, caption = "Log-linaer 'wage' regression output")
#
b1 <- coef(mod5)[[1]]
b2 <- coef(mod5)[[2]]
sighat2 <- smod5$sigma^2
g <- 100*b2 # growth rate
#
yhatn <- exp(b1+b2*xeduc) # 'natural' prediction, small sample
yhatc <- exp(b1+b2*xeduc+sighat2/2) # corrected prediction, large sample
DyDx <- b2*yhatn # marginal effects
#
yhatn; yhatc; DyDx; g
## two predictions comparison
education = seq(0, 22, 2)
yn <- exp(b1+b2*education)
yc <- exp(b1+b2*education+sighat2/2)
plot(cps4_small$educ, cps4_small$wage,
     xlab = "education", ylab = "wage", col = "grey")
lines(yn ~ education, lty = 2, col = "black")
lines(yc ~ education, lty = 3, col = "blue")
legend("topleft", legend = c("normal prediction", "corrected prediction"),
       lty = c(2,3), col = c("black", "blue"),
       x.intersp = 0.3, y.intersp = 0.3, seg.len = 0.8)
## model comparison
mod4 <- lm(wage ~ I(educ^2), cps4_small)
yhat4 <- predict(mod4)
#
yhat5 <- exp(b1+b2*cps4_small$educ+sighat2/2)
rg4 <- cor(cps4_small$wage, yhat4)^2
rg5 <- cor(cps4_small$wage, yhat5)^2
# 
rg4; rg5 # rg4 is better
## forecast interval estimate in the log-linear model
alpha <- 0.05
xeduc <- 12
xedbar <- mean(cps4_small$educ)
df <- df.residual(mod5)
N <- nobs(mod5)
#
tc <- qt(1-alpha/2, df)
varb2 <- vcov(mod5)[2, 2]
#
varf <- sighat2 + sighat2/N + (xeduc - xedbar)^2 * varb2
sef <- sqrt(varf)
lnyhat <- b1+b2*xeduc
lb <- exp(lnyhat-tc*sef)
ub <- exp(lnyhat+tc*sef)
#
lb; ub
##
education <- seq(min(cps4_small$educ), max(cps4_small$educ), 2)
lnyhat <- b1+b2*education
yhat <- exp(lnyhat)
#
varf <- sighat2 + sighat2/N + (education-xedbar)^2 * varb2
sef <- sqrt(varf)
lb <- exp(lnyhat-tc*sef)
ub <- exp(lnyhat+tc*sef)
#
plot(cps4_small$educ, cps4_small$wage, xlab = "education", ylab = "wage", col = "grey")
lines(yhat ~ education, col = "black", lty = 1)
lines(lb ~ education, col = "red", lty = 2)
lines(ub ~ education, col = "red", lty = 2)
legend("topleft", legend = c("yhat", "lb", "ub"), lty = c(1, 2, 2),
       col = c("black", "red", "red"),
       x.intersp = 0.3, y.intersp = 0.3, seg.len = 0.8)
#### The log-log model
data("newbroiler")
View(newbroiler)
#
mod6 <- lm(log(q) ~ log(p), newbroiler)
b1 <- coef(mod6)[[1]]
b2 <- coef(mod6)[[2]] # the increase in real price of fresh chicken by 1% will change the
# quantity demanded by -1.12414%
smod6 <- summary(mod6)
tbl <- round(data.frame(xtable(smod6)), 4)
kable(tbl, caption = "The log-log poultry regression equation")
##
ngrid <- 20
xmin <- min(newbroiler$p)
xmax <- max(newbroiler$p)
step <- (xmax-xmin)/ngrid # get the grid dimension
#
xp <- seq(xmin, xmax, step)
sighat2 <- smod6$sigma ^ 2
yhatc <- exp(b1+b2*log(newbroiler$p)+sighat2/2) # get the fitted value, using corrected
# prediction
yhatc2 <- fitted(mod6) # natural prediction
yc <- exp(b1+b2*log(xp)+sighat2/2) # corrected y value
#
plot(newbroiler$p, newbroiler$q, ylim = c(min(newbroiler$q), max(newbroiler$q)),
     xlab = "price", ylab = "quantity")
lines(yc ~ xp, col = "blue", lty = 2)
# 
rgsq <- cor(newbroiler$q, yhatc)^2 # so the general r square using the corrected fitted
# values
####


